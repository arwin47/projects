{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84794ea5-fcc2-4df3-90c3-4d4f29b90aa1",
   "metadata": {},
   "source": [
    "## RAG Application for quick reference of Appliance user manuals\n",
    "\n",
    "The most reliable source of information for maintenance tips and troubleshooting for any appliance is the accompanied user manual. As the appliance becomes larger and complex, the user manual becomes disproportionately complex. This application aims to help homemakers with quick troubleshooting and maintenance tips without having to search through the several pages of pdf document.\n",
    "\n",
    "\n",
    "### Importing required libraries\n",
    "\n",
    "This tool uses opensource tools like Langchain and Ollama with Llama models to create the RAG pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e14d8407-c4d9-40af-94ea-e9fd24699706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fca11-1c49-4abb-950f-af048073ea1b",
   "metadata": {},
   "source": [
    "### Extracting text from PDF manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90048ab2-f5c1-48f3-b8b5-3f5a011a40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import re\n",
    "\n",
    "def extract_utf8_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        raw_text = page.extract_text()\n",
    "        if raw_text:\n",
    "            # Encode to UTF-8 and ignore non-UTF-8 characters\n",
    "            clean_text = raw_text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "            full_text += clean_text + \"\\n\"\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# Example usage\n",
    "text = extract_utf8_text(\"washing_machine_user_guide.pdf\")    \n",
    "\n",
    "\n",
    "def strip_non_text(text):\n",
    "    # Remove non-printable characters and common placeholders\n",
    "    return re.sub(r\"[^\\x20-\\x7E\\n]\", \"\", text)\n",
    "\n",
    "safe_text = strip_non_text(text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ff239-aad0-4f55-8f34-a77c3ae08e8a",
   "metadata": {},
   "source": [
    "### Collecting the extracted text in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "27453135-8fa0-49fa-bf03-17cdd5ea744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the used pipeline methods are well suited for reading text from file, writing the output to file\n",
    "with open(\"user_guide.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(safe_text)\n",
    "documents = TextLoader(\"user_guide.txt\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cccaa-0d70-430c-b549-128e4d53c31e",
   "metadata": {},
   "source": [
    "### Creating chunks of text\n",
    "The whole text is split into several smaller chunks of text to be fed into vector store database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c57f330-ede5-4210-8fde-353727f971da",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7cea2-6cc6-46bd-a771-04ff357ddaae",
   "metadata": {},
   "source": [
    "### Creating embeddings and storing in Chroma database\n",
    "\n",
    "The chunks of texts are converted into embeddings using `nomic-embed-text` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37bad1a1-a20c-4f5b-badf-1d4403baf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "59a55a59-9c68-475f-94b5-cdc2c50953be",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding=oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a87394-a2c3-4dc2-b7a0-9bf73b1bf999",
   "metadata": {},
   "source": [
    "### Checking the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a110e013-6ee0-43d6-ac19-8bd6671b03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"appliance must be properly grounded\"\n",
    "dc = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412c98b-ddfe-4996-b093-e968782d3868",
   "metadata": {},
   "source": [
    "### Creating chat template with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6754e6aa-a2bc-4e4b-846b-cc60f4f94f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d917f48e-408e-4515-ab21-92e93a587690",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bfa62-9aaf-42e3-a9e1-a42d6e6a35b1",
   "metadata": {},
   "source": [
    "### Using Llama 3.1 model\n",
    "\n",
    "Using `llama3.1:8b` model to be applied on retrieved context from Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c10bd75-7a56-4523-844a-bc7f8b0433bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85bf2-7f25-4fa5-b767-4b9683a9ffc3",
   "metadata": {},
   "source": [
    "### Creating Retriever\n",
    "The reteiver looks up the question and collects the specific chunks of text based on vector embedding similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c8c5b70b-8239-4d35-ab32-cb2dfb601b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461d2c6-4548-4c8c-b246-e08d5044c969",
   "metadata": {},
   "source": [
    "### Function to combine the similarity search results into one chunk of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "149f1c49-e2a0-4b3c-9c12-fc70915c01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99623b2e-1dc6-4a0c-a0c2-384131a6d5e0",
   "metadata": {},
   "source": [
    "### Creating the pipeline\n",
    "The `question` is taken as input is fed into `retriever` retrieves the relevant chunks and the chunks are combined using `format_docs`. The combined chunk is fed into the `prompt` to create template chat which is then passed into the llama3.1 `model`. The output is converted into a usable text format using `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d63f980d-a8dc-41c8-bddc-011d6e9e531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "\n",
    "    {\"context\": retriever | format_docs, \"question\":RunnablePassthrough()}\n",
    "     | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b9a90-5203-4b8b-a58b-c9e352af4d53",
   "metadata": {},
   "source": [
    "### Testing the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d91975b2-d8f6-49b3-98e8-cadfccb67a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how can I help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is 3C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Check the motor for operation and try restarting the cycle.'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Hi, how can I help you?\")\n",
    "q = input()\n",
    "chain.invoke(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
